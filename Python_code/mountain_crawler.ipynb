{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8484db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지:  2 페이지\n",
      "생성url:  ['https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악 사고&start=1', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악 사고&start=11']\n",
      "검색된 기사 갯수: 총  20 개\n",
      "\n",
      "[뉴스 제목]\n",
      "[['세계 최강용병 출신 산악인 두 다리 잃고 에베레스트 ‘의족 점령’', '진천소방서 산악구조훈련', '스위스 서부 산악지역서 관광용 비행기 추락…3명 사망', '계룡소방, 봄철 산악 안전사고 예방 캠페인', '산불 진화용 대형 헬기…행락철 산악 인명 구조 효자 노릇 톡톡', \"천지여성의용소방대, 봄철 산악사고 예방 '등산목 안전지킴이' 운영\", '서삼석 “산악 사고로부터 국민안전 확보”', '광주 서부소방서, 산악 안전사고 예방 캠페인 실시', \"기고 봄철 산악사고 예방 '백신' 알고 가세요\", '고창소방서-119구조대, 산악사고 대비 인명구조훈련'], ['화창한 날씨 충북서 산악사고 잇따라…\"안전수칙 준수\"', '양평소방서, 봄철 산악사고 대비 ‘산악안전지킴이’ 운영', '원주서 사설 구급차 등 3중 추돌사고로 40대 환자 심정지', '봄기운 즐기려다 실족 추락...봄철 산악사고 주의보', '거창소방서, 봄철 산악사고 대비 인명구조 훈련', '봄기운 즐기려다 실족 추락...봄철 산악사고 주의보', '어린시절 불행하면 평생 불행?…산악 고릴라는 달랐다', '광주 서부소방, 봄철 산악 안전사고 캠페인', '포항북부소방서 119구조대, 산악사고 대비 구조훈련 실시', '강화소방서, 항공대와 합동 산악 인명구조훈련']]\n",
      "\n",
      "[뉴스 링크]\n",
      "[['https://www.hani.co.kr/arti/international/international_general/1092729.html', 'http://www.newsis.com/view/?id=NISI20230522_0001271733', 'https://www.yna.co.kr/view/AKR20230521001300109?input=1195m', 'http://www.chungnamilbo.co.kr/news/articleView.html?idxno=717023', 'http://www.kwnews.co.kr/page/view/2023052216455760185', 'http://www.headlinejeju.co.kr/news/articleView.html?idxno=515455', 'http://www.jndn.com/article.php?aid=1684141395360833114', 'https://view.asiae.co.kr/article/2023051511401676810', 'http://www.mediajeju.com/news/articleView.html?idxno=344607', 'http://www.joongdo.co.kr/web/view.php?key=20230519010005755'], ['https://www.yna.co.kr/view/AKR20230512137500064?input=1195m', 'http://www.ifm.kr/news/361344', 'http://www.kwnews.co.kr/page/view/2023051617094376050', 'https://www.ytn.co.kr/_ln/0115_202305140210066190', 'http://fpn119.co.kr/197870', 'https://science.ytn.co.kr/program/view.php?mcd=0082&hcd=&key=202305151155079933', 'https://www.nongmin.com/article/20230518500418', 'http://www.namdonews.com/news/articleView.html?idxno=724648', 'https://www.news1.kr/articles/5043417', 'http://fpn119.co.kr/197878']]\n",
      "\n",
      "[뉴스 내용]\n",
      "검색된 기사 갯수: 총 20 개\n",
      "결과가 news.csv 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "  #입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "    # 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page)\n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "#뉴스기사 내용 크롤링하는 함수 만들기(각 뉴스의 url)\n",
    "def news_contents_crawler(news_url):\n",
    "    contents=[]\n",
    "    for i in news_url:\n",
    "        #각 기사 html get하기\n",
    "        news = requests.get(i)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "            #기사 내용 가져오기 (p태그의 내용 모두 가져오기) \n",
    "        contents.append(news_html.find_all('p'))\n",
    "    return contents\n",
    "#html생성해서 기사크롤링하는 함수 만들기(제목,url): 3개의 값을 반환함(제목, 링크, 내용)\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(i)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과\n",
    "    articles = html.select(\"div.group_news > ul.list_news > li div.news_area > a\")\n",
    "    title = news_attrs_crawler(articles,'title')\n",
    "    url = news_attrs_crawler(articles,'href')\n",
    "    content = news_contents_crawler(url)\n",
    "    return title, url, content #3개의 값을 반환\n",
    "#뉴스크롤링 시작\n",
    "\n",
    "#검색어 입력\n",
    "#search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "search = \"산악 사고\"\n",
    "#검색 시작할 페이지 입력\n",
    "#page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "page =1\n",
    "print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "#검색 종료할 페이지 입력\n",
    "#page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "page2 =2\n",
    "print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")   \n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search,page,page2)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "for i in url:\n",
    "    title, url,content = articles_crawler(url)\n",
    "    news_titles.append(title)\n",
    "    news_url.append(url)\n",
    "    news_contents.append(content)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \",(page2+1-page)*10,'개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(news_url)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "###print(news_contents)\n",
    "\n",
    "import csv\n",
    "\n",
    "# ... 이전 코드 생략 ...\n",
    "\n",
    "# 결과를 저장할 CSV 파일 경로\n",
    "output_file = \"news.csv\"\n",
    "\n",
    "# 검색된 기사 갯수\n",
    "total_articles = (page2 + 1 - page) * 10\n",
    "\n",
    "# CSV 파일에 결과 저장\n",
    "with open(output_file, \"w\", encoding=\"euc-kr\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"뉴스 제목\", \"뉴스 링크\"])  # CSV 파일의 헤더 작성\n",
    "\n",
    "    for i in range(len(news_titles)):\n",
    "        for j in range(len(news_titles[i])):\n",
    "            writer.writerow([news_titles[i][j], news_url[i][j]])  # 뉴스 제목과 링크를 CSV 파일에 추가\n",
    "\n",
    "print(\"검색된 기사 갯수: 총\", total_articles, \"개\")\n",
    "print(\"결과가\", output_file, \"파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
